% \chapter{Sieci neuronowe}
% Lorem ipsum

\section{Sieci neuronowe}
Sieci neuronowe, zwane równie¿ sztucznymi sieciami neuronowymi (ang. Artificial
Neural Networks, ANN), to grupa algorytmów uczenia maszynowego inspirowanych
sposobem przetwarzania informacji przez ludzki mózg. Wyró¿niaj¹ siê zdolnoœci¹
do adaptacji do ró¿nych problemów dziêki procesowi uczenia. Dziêki warstwowej
strukturze i zastosowaniu nieliniowych funkcji aktywacji s¹ w stanie modelowaæ
z³o¿one zale¿noœci w danych i rozwi¹zywaæ problemy, z którymi nie radz¹ sobie
bardziej tradycyjne metody. Ich uniwersalnoœæ sprawia, ¿e znajduj¹ zastosowanie
w ró¿norodnych dziedzinach, takich jak rozpoznawanie obrazów, przetwarzanie
jêzyka naturalnego czy analiza szeregów czasowych.

\subsection{Budowa neuronu}

Podstawowym elementem budowy sieci neuronowej jest sztuczny neuron, który
naœladuje funkcjonowanie biologicznego neuronu. Zadaniem sztucznego neuronu
jest przetwarzanie wartoœci wejœciowych w pojedyncz¹ wartoœæ wyjœciow¹. Neuron
ten sk³ada siê z kilku kluczowych elementów:
% (zob. Rys. \ref{fig/neuron})

\begin{itemize}
    \item Wejœcia (inputs) - ka¿dy neuron odbiera dane wejœciowe, które mog¹ byæ
          wartoœciami liczbowymi pochodz¹cymi z danych lub wynikami dzia³ania innych
          neuronów z poprzedniej warstwy. Ka¿demu wejœciu przypisana jest waga (weight),
          która okreœla znaczenie danego wejœcia dla obliczeñ neuronu.

    \item Blok sumuj¹cy - neuron oblicza sumê wa¿on¹ wejœæ, dodaj¹c równie¿ wartoœæ biasu
          (bias). Bias jest dodatkowym parametrem, który umo¿liwia przesuniêcie funkcji
          aktywacji i lepsze dopasowanie modelu. Wyra¿enie matematyczne opisuj¹ce tê
          operacjê to:
          \[
              z = \sum_{i=1}^n w_i x_i + b
          \]
          gdzie:
          \begin{itemize}
              \item \(w_i\) to wagi wejœæ,
              \item \(x_i\) to wartoœci wejœciowe,
              \item \(b\) to bias,
              \item \(z\) to wynik sumy wa¿onej.
          \end{itemize}

    \item Funkcja aktywacji (activation function) - wynik \(z\) przechodzi przez funkcjê
          aktywacji, która wprowadza nieliniowoœæ do modelu, pozwalaj¹c sieci neuronowej
          uczyæ siê skomplikowanych zale¿noœci. Popularne funkcje aktywacji to:
          \begin{itemize}
              \item ReLU (Rectified Linear Unit): \(\text{ReLU}(z) = \max(0, z)\),
              \item Sigmoidalna: \(\text{sigmoid}(z) = \frac{1}{1 + e^{-z}}\),
              \item Tangens hiperboliczny: \(\text{tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\).
          \end{itemize}
\end{itemize}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[scale=0.6]{Rysunki/neuron-structure.jpg}
%     \caption{Budowa sztucznego neuronu}
%     \caption*{ród³o: \url{https://naadispeaks.blog/2017/11/08/artificial-neural-networks-with-net-in-azure-ml-studio/} [Data uzyskania dostêpu 25.11.2024]}
%     \label{fig/neuron_jpg}
% \end{figure}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
            init/.style={
                    draw,
                    circle,
                    inner sep=2pt,
                    font=\Huge,
                    join = by -latex
                },
            squa/.style={
                    draw,
                    inner sep=2pt,
                    font=\Large,
                    join = by -latex
                },
            start chain=2,node distance=13mm
        ]
        \node[on chain=2]
        (x2) {$x_2$};
        \node[on chain=2,join=by o-latex]
        {$w_2$};
        \node[on chain=2,init] (sigma)
        {$\displaystyle\Sigma$};
        \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activate \\ function}}]
        {$f$};
        \node[on chain=2,label=above:Output,join=by -latex]
        {$y$};
        \begin{scope}[start chain=1]
            \node[on chain=1] at (0,1.5cm)
            (x1) {$x_1$};
            \node[on chain=1,join=by o-latex]
            (w1) {$w_1$};
        \end{scope}
        \begin{scope}[start chain=3]
            \node[on chain=3] at (0,-1.5cm)
            (x3) {$x_3$};
            \node[on chain=3,label=below:Weights,join=by o-latex]
            (w3) {$w_3$};
        \end{scope}
        \node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};

        \draw[-latex] (w1) -- (sigma);
        \draw[-latex] (w3) -- (sigma);
        \draw[o-latex] (b) -- (sigma);

        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
    \end{tikzpicture}
    \caption{Budowa sztucznego neuronu}
    \caption*{ród³o: \url{https://tex.stackexchange.com/questions/132444/diagram-of-an-artificial-neural-network} [Data uzyskania dostêpu 25.11.2024]}
    %     \caption*{ród³o: \url{https://naadispeaks.blog/2017/11/08/artificial-neural-networks-with-net-in-azure-ml-studio/}
    \label{fig/neuron}
\end{figure}

\subsection{Klasyczna budowa sieci neuronowej}

Struktura sieci neuronowej sk³ada siê z grup sztucznych neuronów, zwanych
warstwami. Sieæ sk³ada siê z trzech g³ównych typów warstw: warstwy wejœciowej,
jednej lub wiêcej warstw ukrytych oraz warstwy wyjœciowej. Ka¿da z tych warstw
pe³ni okreœlon¹ funkcjê w przetwarzaniu danych, a ich odpowiednie po³¹czenie
umo¿liwia rozwi¹zywanie z³o¿onych problemów z zakresu klasyfikacji, regresji
czy analizy danych.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
            plain/.style={
                    draw=none,
                    fill=none,
                },
            net/.style={
                    matrix of nodes,
                    nodes={
                            draw,
                            circle,
                            inner sep=10pt
                        },
                    nodes in empty cells,
                    column sep=2cm,
                    row sep=-9pt
                },
            >=latex
        ]
        \matrix[net] (mat)
        {
        |[plain]| \parbox{1.3cm}{\centering Input\\layer} & |[plain]| \parbox{1.3cm}{\centering Hidden\\layer} & |[plain]| \parbox{1.3cm}{\centering Output\\layer} \\
        & |[plain]| \\
        |[plain]| & \\
        & |[plain]| \\
        |[plain]| & |[plain]| \\
        & & \\
        |[plain]| & |[plain]| \\
        & |[plain]| \\
        |[plain]| & \\
        & |[plain]| \\    };
        \foreach \ai [count=\mi ]in {2,4,...,10}
        \draw[<-] (mat-\ai-1) -- node[above] {Input \mi} +(-2cm,0);
        \foreach \ai in {2,4,...,10}
            {\foreach \aii in {3,6,9}
                \draw[->] (mat-\ai-1) -- (mat-\aii-2);
            }
        \foreach \ai in {3,6,9}
        \draw[->] (mat-\ai-2) -- (mat-6-3);
        \draw[->] (mat-6-3) -- node[above] {Ouput} +(2cm,0);
    \end{tikzpicture}
    \caption{Uogólniona budowa sieci neuronowej}
    \caption*{ród³o: \url{https://tex.stackexchange.com/questions/132444/diagram-of-an-artificial-neural-network} [Data uzyskania dostêpu 25.11.2024]}
    %     \caption*{ród³o: \url{https://naadispeaks.blog/2017/11/08/artificial-neural-networks-with-net-in-azure-ml-studio/}
    \label{fig/neural_network_basic_scheme}
\end{figure}

\subsubsection{Warstwa wejœciowa}
Warstwa wejœciowa to pierwsza czêœæ sieci neuronowej, której g³ównym zadaniem
jest odbiór danych wejœciowych. Ka¿dy neuron w tej warstwie reprezentuje jedn¹
cechê z danych wejœciowych, co oznacza, ¿e liczba neuronów w warstwie
wejœciowej jest równa liczbie zmiennych lub parametrów w analizowanym zbiorze
danych. Na przyk³ad w przypadku analizy sygna³u EKG, ka¿dy neuron w warstwie
wejœciowej mo¿e odpowiadaæ pojedynczej próbce sygna³u lub cechom wyodrêbnionym
z tego sygna³u, takim jak wartoœæ amplitudy w danym momencie.

Neurony w warstwie wejœciowej nie wykonuj¹ ¿adnych operacji matematycznych. Ich
rola ogranicza siê do przekazania otrzymanych danych do nastêpnych warstw w
sieci. Dane te s¹ nastêpnie przekszta³cane i analizowane w warstwach ukrytych.

\subsubsection{Warstwy ukryte}
Warstwy ukryte stanowi¹ centraln¹ czêœæ sieci neuronowej, gdzie odbywa siê
wiêkszoœæ przetwarzania danych. W warstwach tych neurony s¹ po³¹czone z
neuronami z poprzedniej i nastêpnej warstwy, a ka¿de po³¹czenie ma przypisan¹
wagê. Wagi te s¹ kluczowe, poniewa¿ okreœlaj¹, jak silny wp³yw ma dany neuron
na aktywacjê innych neuronów.

Ka¿dy neuron w warstwie ukrytej wykonuje typowe operacje dla sztucznego
neuronu: oblicza sumê wa¿on¹ swoich wejœæ, dodaje wartoœæ biasu, a nastêpnie
przekszta³ca wynik za pomoc¹ funkcji aktywacji. Funkcja aktywacji wprowadza
nieliniowoœæ, co pozwala sieci neuronowej uczyæ siê bardziej z³o¿onych wzorców
w danych. Szczegó³y dotycz¹ce tych operacji zosta³y opisane w podrozdziale
dotycz¹cym budowy neuronu.

Liczba warstw ukrytych oraz liczba neuronów w ka¿dej warstwie jest parametrem
sieci, który dobiera siê w zale¿noœci od z³o¿onoœci problemu. G³êbokie sieci
neuronowe, które posiadaj¹ wiele warstw ukrytych, s¹ zdolne do automatycznego
wyodrêbniania i przetwarzania z³o¿onych cech danych, co czyni je efektywnymi w
takich zadaniach jak rozpoznawanie wzorców w sygnale EKG czy przewidywanie
wartoœci zmiennych opisuj¹cych HRV i RSA.

Warto jednak zaznaczyæ, ¿e wiêksza liczba warstw ukrytych lub neuronów w sieci
nie zawsze oznacza lepsze rezultaty. Zwiêkszanie liczby warstw lub neuronów
mo¿e prowadziæ do problemów, takich jak przeuczenie (ang. overfitting), gdzie
sieæ zbyt dok³adnie dopasowuje siê do danych treningowych kosztem generalizacji
na nowych danych. Ponadto, wiêksza liczba warstw oznacza wy¿sze wymagania
obliczeniowe i wiêksz¹ z³o¿onoœæ optymalizacji.

\subsubsection{Warstwa wyjœciowa}
Warstwa wyjœciowa jest ostatni¹ warstw¹ sieci neuronowej. Jej zadaniem jest
przekszta³cenie wyników przetwarzania uzyskanych w warstwach ukrytych na postaæ
wynikow¹, zgodn¹ z celem analizy. Liczba neuronów w warstwie wyjœciowej zale¿y
od rodzaju problemu. Na przyk³ad w zadaniach klasyfikacyjnych liczba neuronów
odpowiada liczbie klas, natomiast w zadaniach regresji warstwa wyjœciowa sk³ada
siê z jednego neuronu, który przewiduje wartoœæ liczbow¹.

Wartoœci w warstwie wyjœciowej s¹ czêsto poddawane dodatkowym przekszta³ceniom,
takim jak funkcja softmax w przypadku klasyfikacji wieloklasowej lub funkcja
liniowa w zadaniach regresyjnych. Ostateczne wyniki s¹ nastêpnie porównywane z
oczekiwanymi wynikami (danymi referencyjnymi), co umo¿liwia obliczenie b³êdu i
optymalizacjê wag w sieci.

\subsubsection{£¹czenie warstw i propagacja danych}
Sieci neuronowe dzia³aj¹ na zasadzie przep³ywu informacji pomiêdzy warstwami.
Dane wejœciowe przechodz¹ przez warstwê wejœciow¹ do warstw ukrytych, gdzie s¹
przetwarzane przez neurony. Wyniki tego przetwarzania trafiaj¹ do warstwy
wyjœciowej, która generuje koñcowy wynik. Proces ten nazywa siê propagacj¹ w
przód (ang. forward propagation).

Podczas treningu sieci neuronowej stosuje siê algorytm wstecznej propagacji
b³êdu (ang. backpropagation), który umo¿liwia aktualizacjê wag po³¹czeñ miêdzy
neuronami na podstawie ró¿nicy miêdzy wynikiem przewidzianym przez sieæ a
oczekiwanym wynikiem. Dziêki temu sieæ uczy siê dostosowywaæ swoje parametry,
aby lepiej modelowaæ zale¿noœci w danych.

\subsection{Sieci konwolucyjne}

Sieci konwolucyjne (ang. Convolutional Neural Networks, CNN) to szczególny
rodzaj sieci neuronowych, zaprojektowany do analizy danych o strukturze
przestrzennej, takich jak obrazy, sygna³y czasowe czy dane
przestrzenno-czasowe. W odró¿nieniu od klasycznych sieci neuronowych, CNN
wykorzystuj¹ operacje konwolucji, które umo¿liwiaj¹ ekstrakcjê lokalnych cech
danych, jednoczeœnie redukuj¹c liczbê parametrów i zachowuj¹c istotne
informacje.

Budowa sieci konwolucyjnych ró¿ni siê od klasycznych sieci neuronowych poprzez
wprowadzenie nowych typów warstw: warstw konwolucyjnych, warstw spajaj¹cych
(ang. pooling layers) oraz warstw w pe³ni po³¹czonych (ang. fully connected
layers). Te elementy pozwalaj¹ sieci konwolucyjnej efektywnie analizowaæ dane
przestrzenne.

\subsubsection{Warstwa konwolucyjna}
Warstwa konwolucyjna jest kluczowym elementem CNN. Jej zadaniem jest wykrywanie
lokalnych wzorców w danych wejœciowych poprzez operacjê konwolucji. W warstwie
tej stosuje siê filtry (zwane równie¿ j¹drami lub kernelami), które s¹
macierzami o niewielkich rozmiarach (np. 3x3, 5x5, 3x1, 5x1). Ka¿dy filtr
przesuwa siê po danych wejœciowych (np. obrazie lub sygnale), obliczaj¹c
iloczyn skalarny wartoœci w filtrze i fragmentu danych wejœciowych. Wynikiem
jest mapa cech (ang. feature map), która reprezentuje wykryte wzorce. Wyra¿enie
matematyczne opisuj¹ce konwolucjê dla danych 2-wymiarowych mo¿na zapisaæ jako:
\begin{equation}
    S(i, j) = \sum_{m=1}^M \sum_{n=1}^N I(m, n)K(i-m, j-n)
\end{equation}
gdzie:
\begin{itemize}
    \item \(I(m, n)\) to dane wejœciowe,
    \item \(K(i, j)\) to wartoœci filtra,
    \item \(S(i, j)\) to wynik konwolucji w danym miejscu.
    \item \(M\) i \(N\) to wymiary filtra
\end{itemize}
Splot jest operacj¹ przemienn¹, wiêc mo¿na to równanie zapisaæ w postaci
\begin{equation}
    S(i, j) = \sum_{m=1}^M \sum_{n=1}^N I(i-m, j-n)K(m, n)
\end{equation}
Warstwa konwolucyjna w sieciach neuronowych zwykle implementuje operacjê korelacji krzy¿owej (ang. cross-correlation), która matematycznie jest bardzo zbli¿ona do klasycznej konwolucji, ale nie odwraca filtra (j¹dra) przed obliczeniami \cite{Goodfellow-DeepLearning}.
Wyra¿enie opisuj¹ce korelacjê krzy¿ow¹ to:
\begin{equation}
    S(i, j) = \sum_{m=1}^M \sum_{n=1}^N I(i+m, j+n)K(m, n)
\end{equation}
Analogicznie, operacje korelacji krzy¿owej na danych 1-wymiarowych opisuje równanie:
\begin{equation}
    S(i) = \sum_{m=1}^M I(i+m)K(m)
\end{equation}
Operacja splotu w warstwie konwolucyjnej jest odpowiednikiem obliczenia sumy wa¿onej w bloku sumuj¹cym klasycznego sztucznego neuronu. Wynik tej operacji, podobnie jak w klasycznym neuronie, jest przekazywany do funkcji aktywacji w celu wprowadzenia nieliniowoœci.\\

Dzia³anie warstwy konwolucyjnej mo¿na dostosowaæ poprzez odpowiedni¹
konfiguracjê jej hiperparametrów. Do najwa¿niejszych z nich nale¿¹ m.in.
\begin{itemize}
    \item Przesuniêcie filtru (stride) - okreœla liczbê pozycji, o jakie filtr przesuwa
          siê w danych wejœciowych. Zwiêkszenie wartoœci tego parametru zmniejsza rozmiar
          map cech, co obni¿a koszt obliczeniowy, jednak mo¿e skutkowaæ utrat¹ drobnych
          informacji.
    \item Dope³nianie zerami (padding) - umo¿liwia zachowanie informacji na krawêdziach
          danych wejœciowych. Bez dope³niania mapa cech ulega naturalnemu zmniejszeniu.
    \item Rozmiar filtra (kernel size) - okreœla wymiary filtra. Mniejsze filtry
          wykrywaj¹ drobniejsze wzorce, wiêksze analizuj¹ szerszy kontekst danych.
    \item Liczba filtrów - decyduje o liczbie generowanych map cech, co zwiêksza zdolnoœæ
          sieci do wykrywania ró¿nych wzorców.
\end{itemize}

\subsubsection{Warstwa spajaj¹ca (pooling layer)}
Warstwa spajaj¹ca (ang. pooling layer) zmniejsza wymiary danych wejœciowych,
zachowuj¹c kluczowe informacje. Redukuje liczbê parametrów modelu, co zapobiega
przeuczeniu i zwiêksza wydajnoœæ obliczeñ. Najczêœciej stosowane operacje to:
\begin{itemize}
    \item \textit{Max pooling} – wybiera maksymaln¹ wartoœæ z fragmentu danych wejœciowych, wykrywaj¹c najbardziej znacz¹ce cechy.
    \item \textit{Average pooling} – oblicza œredni¹ wartoœæ w analizowanym fragmencie, zachowuj¹c informacje o ogólnym kontekœcie danych.
\end{itemize}
Warstwa ta u³atwia modelowi analizê wzorców, zmniejszaj¹c jednoczeœnie wra¿liwoœæ na drobne przesuniêcia w danych.

\subsubsection{Warstwa w pe³ni po³¹czona (fully connected layer)}
Ostateczna analiza przetworzonych cech odbywa siê w warstwach w pe³ni
po³¹czonych. Wyniki uzyskane z warstw konwolucyjnych i spajaj¹cych s¹
przekszta³cane w jeden wymiar, a nastêpnie przechodz¹ przez standardowe neurony
w celu uzyskania finalnych wyników, takich jak klasyfikacja czy predykcja
wartoœci liczbowych.

\subsubsection{Ró¿nice w warstwie wejœciowej CNN w porównaniu z klasycznymi sieciami neuronowymi}
Warstwa wejœciowa w sieciach konwolucyjnych ró¿ni siê istotnie od warstwy
wejœciowej opisanej w podrodziale dotycz¹cym klasycznej budowy sieci
neuronowej. Ro¿nice wystêpuj¹ g³ównie pod wzglêdem sposobu przetwarzania i
reprezentacji danych. W klasycznych sieciach neuronowych dane wejœciowe s¹
przekszta³cane w jednowymiarowy wektor, niezale¿nie od pierwotnej struktury
danych. Przyk³adowo obraz o wymiarach 100x100 jest zamieniany w wektor o
dlugosci 10000 elementów. Taka reprezentacja prowadzi do utraty informacji o
przestrzennych zale¿noœciach pomiêdzy poszczególnymi elementami danych.

W CNN warstwa wejœciowa zachowuje przestrzenn¹ strukturê danych, co pozwala na
analizê lokalnych wzorców w ich oryginalnym kontekœcie. Na przyk³ad, ten sam
obraz 100x100 pozostaje macierz¹ o tych samych wymiarach, a w przypadku obrazów
kolorowych uwzglêdniany jest dodatkowy wymiar g³êbokoœci, np. 3 kana³y
dla RGB. Dziêki temu konwolucje wykonywane przez kolejne warstwy sieci mog¹
wykrywaæ lokalne cechy, takie jak krawêdzie, tekstury czy inne bardziej z³o¿one
wzorce.

Ponadto, CNN wprowadza mechanizm wspó³dzielenia wag w warstwach konwolucyjnych,
co dodatkowo redukuje liczbê parametrów, zwiêkszaj¹c efektywnoœæ i zdolnoœæ
uogólniania modelu. W klasycznych sieciach ka¿da cecha wejœciowa ma swoj¹
unikaln¹ wagê, co przy du¿ej liczbie danych wejœciowych prowadzi do znacz¹cego
wzrostu liczby parametrów.

