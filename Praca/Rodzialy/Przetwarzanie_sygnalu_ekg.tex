% \chapter{Implementacja algorytmów do analizy EKG}
% Lorem ipsum
\section{Wyznaczanie szczytów R}
\subsection{Pan-Tompkins}
Algorytm Pan-Tomkins jest powszechnie uznawanym narzêdziem do wykrywania
za³amków R w sygna³ach EKG, zapewniaj¹c przy tym wysok¹ precyzjê. Algorytm
sk³ada sie z kilku kluczowych etapów przetwarzania sygna³u: filtracja
œrodkowoprzepustowa, ró¿niczkowanie, kwadratowanie, okno ca³kuj¹ce, ustalenie
progów i detekcja szczytów R \cite{Fariha-PanTompkins}. Dodatkowo, aby
dok³adnie dopasowaæ wykryte szczyty do rzeczywistych najwy¿szych wartoœci
sygna³u, wprowadzono dodatkowy krok, polegaj¹cy na sprawdzaniu otoczenia
wykrytego szczytu.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{Rysunki/Pan-Tomkins-scheme.png}
    \caption{Schemat algorytmu Pan-Tompkins z dodatkowym krokiem}
    \label{fig/PanTompkinsFlowChart}
\end{figure}

% \subsection{Filtracja œrodkowoprzepustowa}
Filtracja œrodkowoprzepustowa ma na celu usuniêcie zak³óceñ wystêpuj¹cych w
sygnale, co za tym idzie zwiêkszenie stosunku sygna³u do szumu
\cite{Fariha-PanTompkins}. Algorytm Pan-Tompkins standardowo wykorzystuje
filtracjê w paœmie 5–15 Hz \cite{Fariha-PanTompkins}. Jednak na wczesnym etapie
realizacji projektu zaobserwowano, ¿e zastosowanie szerszego pasma filtracji
5-18 Hz, zaproponowanego w pracy \cite{Khan-PanTompkins++}, prowadzi³o do nieco
lepszych rezultatów.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{Rysunki/Pan Tompkins_raw.png}
    \caption{Fragment surowego sygna³u EKG z czujnika Polar H10}
    \label{fig/PanTompkinsRaw}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{Rysunki/Pan_tomkins_5_15.png}
    \caption{Fragment przefiltrowanego sygna³u EKG z czujnika Polar H10}
    \label{fig/PanTompkinsFiltered}
\end{figure}

% \subsection{Ró¿niczkowanie}
Po wstêpnym przefiltrowaniu sygna³ EKG jest poddawany ró¿niczkowaniu, co ma na
celu uwypuklenie szybkoœci zmian w sygnale, co pozwala na lepsze wyró¿nienie
charakterystycznych punktów, takich jak szczyty R. W procesie wyznaczania
pochodnej nisko-czêstotliwoœciowe sk³adowe sygna³u s¹ t³umione, a
gwa³towniejsze zmiany uwypuklane, skutkuje to m. in. st³umieniem fal P i T oraz
wzmocnieniem bardziej stromych zbocz w kompleksie QRS
\cite{Fariha-PanTompkins}.\\

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{Rysunki/PanTompkins_derivative.png}
    \caption{Fragment sygna³u EKG z czujnika Polar H10 po ró¿niczkowaniu}
    \label{fig/PanTompkinsDerivative}
\end{figure}

% \subsection{Kwadratowanie}
Kwadratowanie jest kolejnym etapem, w którym sygna³ uzyskany w poprzednim kroku
zostaje podniesiony do kwadratu. Dziêki temu wszystkie wartoœci sygna³u staj¹
siê dodatnie, a fragmenty odpowiadaj¹ce kompleksowi QRS staj¹ siê bardziej
widoczne. Dodatkowo, proces kwadratowania wyraŸniej uwypukla ró¿nicê miêdzy
kompleksem QRS a fal¹ T, co pozwala na ich lepsze rozró¿nienie i redukuje
ryzyko fa³szywych wykryæ szczytów R \cite{Fariha-PanTompkins}. \\

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{Rysunki/Pan_tomkins_squared.png}
    \caption{Fragment sygna³u EKG z czujnika Polar H10 po ró¿niczkowaniu}
    \label{fig/PanTompkinsSquared}
\end{figure}

% \subsection{Okno ca³kuj¹ce}
Okno ca³kuj¹ce ma na celu wyg³adzenie i uœrednienie sygna³u w okreœlonym
przedziale, oknie czasowym. Proces ten polega na obliczaniu sumy wartoœci
próbek sygna³u w ramach przesuwaj¹cego sie okna, co prowadzi do uzyskania
bardziej spójnego i wyg³adzonego sygna³u wyjœciowego. W ten sposób uwypuklane
s¹ d³ugotrwa³e trendy i zmiany sygna³u, co znacznie u³atwia wykrywanie
charakterystycznych za³amków QRS. Okno ca³kuj¹ce pomaga równie¿ w eliminacji
mniejszych zak³óceñ, które mog³yby powodowaæ fa³szywe wykrycia szczytów,
zwiêkszaj¹c precyzjê i wiarygodnoœæ procesu detekcji. Dziêki zastosowaniu tego
etapu mo¿liwe jest uzyskanie lepszej separacji szczytów R od pozosta³ych
elementów sygna³u. Dla danych pochodz¹cych z czujnika Polar H10 zastosowano
okno o szerokoœci 150 ms \cite{Fariha-PanTompkins}, co przek³ada³o siê na oko³o
20 próbek, bior¹c pod uwagê czêstotliwoœæ próbkowania czujnika: 130 Hz.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{Rysunki/Pan_tomkins_integrated.png}
    \caption{Fragment sygna³u EKG z czujnika Polar H10 po oknie ca³kuj¹cym}
    \label{fig/Pan_tomkins_MWI}
\end{figure}

% \subsection{Ustalenie progu i detekcja szczytów R}
Ustalenie progu ma na celu detekcjê szczytów R, przy jednoczesnej minimalizacji
fa³szywych wykryæ. W tym celu ustalany jest próg na bazie którego podejmowana
jest decyzja. Próg ten mo¿e byæ wartoœci¹ sta³¹ lub byæ dynamicznie
dostosowywany, aby uwzglêdniæ zmiany amplitudy w ró¿nych odcinkach sygna³u.
Podczas oznaczania danych pochodz¹cych z czujnika Polar H10 zastosowano próg
obliczany jako œrednia wartoœæ sygna³u ca³kowanego powiêkszona o 0,6-krotnoœæ
odchylenia standardowego tego sygna³u.\\

Podczas procesu detekcji wartoœci sygna³u porównywane s¹ z ustalonym progiem.
Gdy sygna³ przekroczy wartoœæ progu, wstêpnie identyfikowany jest jako
potencjalny szczyt R. W celu minimalizacji fa³szywych wykryæ stosuje siê
dodatkowe regu³y, takie jak minimalny odstêp czasowy pomiêdzy wykrytymi
szczytami. Podczas analizy danych pochodz¹cych z czujnika Polar H10 zastosowano
odstêp wynosz¹cy 400 ms.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{Rysunki/Pan Tomkins without refining.png}
    \caption{Fragment sygna³u EKG z czujnika Polar H10 po wykryciu szczytów}
    \label{fig/Pan_tomkins_peaks}
\end{figure}

% \subsection{Doprecyzowanie pozycji szczytów}
W dodatkowym kroku przeanalizowano otoczenie wykrytych szczytów w pierwotnym
sygnale EKG w celu zwiêkszenia precyzji detekcji. Bez tej dodatkowej analizy
wykryte za³amki czêsto by³y lekko przesuniête wzglêdem faktycznego maksimum
sygna³u. Dlatego dla ka¿dego wykrytego punktu zbadano jego otoczenie i jako
szczyt oznaczono wartoœæ maksymaln¹ w tym zakresie. W przypadku danych
pochodz¹cych z czujnika Polar H10 rozmiar okna, w którym poszukiwano wartoœci
maksymalnej, zosta³ ustalony na 10 próbek w obu kierunkach.\\

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{Rysunki/Pan_tomkins_final.png}
    \caption{Fragment sygna³u EKG z czujnika Polar H10 po doprecyzowaniu pozycji szczytów}
    \label{fig/Pan_tomkins_refined_peaks}
\end{figure}

\subsection{Sieæ UNet}

% Nastêpn¹ metoda wykorzystana do wyznaczenia pozycji sczytów R w niniejszej pracy jest sieæ
% neuronowa oparta o architekturê 1D UNet, zaczerpniêta z pracy
% \cite{MUzairZahid-UNET}. Schemat budowy sieci, wraz z blokiem postprocessingu i
% weryfikacji, przedstawiono na Rys. \ref{fig/UNet}, pochodz¹cym z tej samej
% publikacji. Caloœæ zosta³a zrealizowana przy u¿yciu biblioteki TensorFlow, z
% wykorzystaniem jej wysokopoziomowego API o nazwie Keras
% \cite{TensorFlow-Keras}.

Kolejn¹ metod¹ wykorzystan¹ w niniejszej pracy do wyznaczenia pozycji za³amków
R jest sieæ neuronowa oparta na architekturze 1D U-Net, zaczerpniêta z pracy
\cite{MUzairZahid-UNET}.
% Schemat budowy sieci, uwzglêdniaj¹cy blok
% postprocessingu i weryfikacji, przedstawiono na rysunku \ref{fig/UNet}, który
% pochodzi z tej samej publikacji. 
Sieæ opiera siê na modelu koder-dekoder, sk³adaj¹cym siê z dwóch g³ównych
elementów: œcie¿ki koduj¹cej i œcie¿ki dekoduj¹cej. Implementacja w projekcie
in¿ynierskim bazuje na kodzie udostêpnionym przez autorów, jednak zosta³a
zmodyfikowana i dostosowana do potrzeb projektu. Ca³oœæ zrealizowano przy
u¿yciu biblioteki TensorFlow oraz jej wysokopoziomowego API o nazwie Keras
\cite{TensorFlow-Keras}. Sschemat budowy sieci, pochodz¹cy z tej samej
publikacji, przedstawiono na Rys \ref{fig/UNet}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{Rysunki/U-net_small.png}
    \caption{Schemat sieci U-Net}
    \label{fig/UNet}
\end{figure}

% Œcie¿ka koduj¹ca odpowiada za ekstrakcje istotnych cech sygna³u. Œcie¿ke koduj¹c¹ z³o¿ono z 6 warstw. Na ka¿da warstwê tej œcie¿ki sk³ada siê warstwa splotowa i funkcja aktywacji LeakyRELU. Warstwa splotowa dokonuje jednoczeœnie operacji splotu i dwukrotnej redukcji wektora wejœciowego, poprzez parametr stride, definiuj¹cego rozmiar kroku przesuwania filtra. 

Œcie¿ka koduj¹ca w modelu zosta³a zaprojektowana w celu ekstrakcji kluczowych cech sygna³u wejœciowego oraz jego kontrolowanej redukcji. Sk³ada siê z szeœciu warstw, z których ka¿da realizuje przetwarzanie danych poprzez operacje splotu oraz funkcjê aktywacji LeakyReLU.

Ka¿da warstwa wykorzystuje jednowymiarow¹ warstwê splotow¹ (Conv1D), która
wykonuje operacjê konwolucji na danych wejœciowych, umo¿liwiaj¹c wyodrêbnienie
istotnych cech sygna³u. Warstwa ta jednoczeœnie zmniejsza d³ugoœæ wektora
wyjœciowego o po³owê dziêki zastosowaniu parametru stride=2, który definiuje
krok przesuwania filtra. Aby zachowaæ pe³n¹ informacjê w ca³ym zakresie
sygna³u, zastosowano parametr padding='same'. Dziêki temu wyjœcie uwzglêdnia
ca³¹ d³ugoœæ wejœcia, niezale¿nie od rozmiaru filtra, a brakuj¹ce wartoœci s¹
uzupe³niane zerami. Ostateczny rozmiar wyjœcia jest kontrolowany przez parametr
stride, który decyduje o stopniu redukcji danych. Liczba filtrów oraz ich
rozmiar zmieniaj¹ siê co dwie warstwy i wynosz¹ odpowiednio: 16, 32, 64 dla
liczby filtrów oraz 9, 6, 3 dla rozmiaru filtra.\\

Œcie¿ka dekoduj¹ca odpowiada za rekonstrukcjê sygna³u wyjœciowego na podstawie zakodowanych cech dostarczonych przez œcie¿kê koduj¹c¹. Sk³ada siê z szeœciu warstw transponowanych splotów (Conv1DTranspose), które realizuj¹ operacje odwrotne do warstw koduj¹cych. Warstwy te zwiêkszaj¹ d³ugoœæ wektora wyjœciowego dwukrotnie dziêki zastosowaniu parametru stride=2, co pozwala na stopniowe przywracanie pierwotnego rozmiaru sygna³u wejœciowego. Podobnie jak w œcie¿ce koduj¹cej, zastosowano parametr padding='same', aby zachowaæ zgodnoœæ d³ugoœci wyjœcia na ka¿dym etapie rekonstrukcji. W pierwszej warstwie dekoduj¹cej zastowsowano równie¿ mechanizm dropout z prawdopodobienstwem 0.25, w celu zignorowania pewnej liczby neuronów i poprawy generalizacji sieci. Ka¿da warstwa dekoduj¹ca wykorzystuje funkcjê aktywacji LeakyReLU, z wyj¹tkiem ostatniej, gdzie zastosowano funkcjê sigmoid, co wprowadza nieliniowoœæ do procesu dekodowania. Ostatnia wartstwa wykorzystuje wy³¹cznie jeden filtr, w celu wygenerowania jednego kana³u wyjœciowego.
Wynikiem przetwarzania fragmentu sygna³u przez model jest jednowymiarowa mapa segmentacji, która przedstawia prawdopodobieñstwo ka¿dej próbki sygna³u na bycie szczytem R.

Kazda z warstw modelu, z wyj¹tkiem pierwszej, korzysta równie¿ z normalizacji
wsadowej (BatchNormalization), co pozwala na stabilizacje oraz przyspieszenie
procesu uczenia poprzez standaryzacje danych.

W podanym modelu zastosowano po³¹czenia skip connections, które umo¿liwiaj¹
po³¹czenie wyjœæ z warstw koduj¹cych z danymi przetwarzanymi w œcie¿ce
dekoduj¹cej. W trakcie przechodzenia sygna³u przez œcie¿kê koduj¹c¹, wyjœcia z
ka¿dej warstwy koduj¹cej s¹ zapisywane na osobnej liœcie. W œcie¿ce dekoduj¹cej
te zapisane wyjœcia z warstw koduj¹cych s¹ póŸniej wykorzystywane do po³¹czenia
z aktualnymi danymi przetwarzanymi w dekoderze. Po³¹czenie to realizowane jest
za pomoc¹ operacji konkatenacji (Concatenate), która ³¹czy dwa zestawy danych –
wyjœcie z warstwy dekoduj¹cej oraz odpowiadaj¹ce mu wyjœcie z warstwy koduj¹cej
– wzd³u¿ osi kana³ów. Powoduje to, ¿e dekoder korzysta z bogatszego zestawu
informacji, zawieraj¹cego zarówno cechy szczegó³owe, jak i bardziej ogólne. To
po³¹czenie pozwala na dok³adniejsz¹ rekonstrukcjê sygna³u i unikniêcie utraty
szczegó³ów w procesie kodowania.\\

Wynik uzyskany z modelu wymaga dodatkowego przetwarzania, aby precyzyjnie
zidentyfikowaæ rzeczywiste za³amki R. Ca³y proces wykrywania tych szczytów, od
sygna³u do wyników, przebiega w nastêpuj¹cych etapach:
\begin{enumerate}
    \item Podzia³ sygna³u na okna: Sygna³ jest dzielony na nachodz¹ce na siebie okna. W
          tej pracy zastosowano parametr stride równy 3/4 wielkoœci pojedynczego okna, co
          zapewnia czêœciowe nak³adanie siê okien.
    \item Przetwarzanie przez model: Ka¿de okno jest przepuszczane przez model, który
          generuje mapê prawdopodobieñstw dla próbek w tym oknie.
    \item Scalanie wyników: Uzyskane wyniki z nak³adaj¹cych siê okien s¹ ³¹czone i
          uœredniane, co redukuje b³êdy na granicach okien i pozwala uzyskaæ jedn¹,
          spójn¹ mapê prawdopodobieñstw dla ca³ego sygna³u.
    \item Selekcja punktów: Na podstawie zdefiniowanego progu (w tym przypadku threshold
          = 0.5) wybierane s¹ punkty o wysokim prawdopodobieñstwie bycia szczytem R.
    \item Korekcja punktów: Wybrane punkty s¹ przesuwane w kierunku wartoœci, która jest
          najbardziej oddalona od œredniej wyg³adzonego sygna³u. Proces ten realizowany
          jest za pomoc¹ funkcji \emph{correct\textunderscore peaks} z modu³u
          \emph{wfdb.processing}
    \item Finalne rozpoznanie szczytów R: Jeœli co najmniej piêæ punktów zostanie
          podci¹gniête do tego samego miejsca, punkt ten zostaje zaklasyfikowany jako
          szczyt R.
\end{enumerate}\

\begin{figure}[ht]
    \centering
    \includegraphics[scale=1]{Rysunki/unet-postprocess.png}
    \caption{Uproszczony schemat przetworzenia wyników sieci, od otrzymanych map segmentacji z sieci do rozpoznania szczytu}
    \label{fig/UNet-r-preak-identification}
\end{figure}

W ramach modyfikacji wzglêdem pierwotnej implementacji zmniejszono rozmiar okna
(autorzy zalecali zakres od 5 do 20 sekund) do oko³o 1,5 sekundy, co umo¿liwi³o
zastosowanie algorytmu w czasie rzeczywistym. Ponadto zmieniono sposób korekty
za³amków R — w pierwotnej wersji korekta by³a zawsze wykonywana do wartoœci
najwy¿szej, natomiast wprowadzone zmiany pozwalaj¹ na korektê do wartoœci
najbardziej oddalonej od œredniej. Udoskonalenie to umo¿liwi³o poprawne
rozpoznawanie za³amków R nawet w przypadku odwrócenia sygna³u EKG, na przyklad
z powodu zamiany polaryzacji elektrod. Metodê obudowano w³asnym interfejsem,
który by³ wspomniany w podsekcji \ref{subsec:wykrywacz} \FloatBarrier
\section{Modele neuronowe do wyznaczania miar HRV}
\subsection{Model oparty o interwa³y RR}\label{rr-based}
% \subsection{Model do wyznaczania miar HRV na bazie interwa³ów RR}
W trakcie prac nad modelem end-to-end opracowano model poœredni, który
przewiduje miary HRV na podstawie interwa³ów RR. Pocz¹tkowo planowano jego
intergracje z modelem UNet w jedn¹ sieæ typu end-to-end, jednak ostatecznie
zrezygnowano z tego podejœcia. Model end-to-end zosta³ stworzony jako
bezpoœrednie rozwiniêcie tego modelu. Do implementacji modelu wykorzystano
wysokopoziomowe API biblioteki \emph{tensorflow} o nazwie \emph{Keras}
\cite{TensorFlow-Keras}. Schemat modelu poœredniego przedstawiono na Rys.
\ref{fig/rr_siec}.

Model oparto o kaskade trzech bloków przetwarzania, z których ka¿dy zawiera
jednowymiarowa warstwê splotow¹ (Conv1D), normalizacje wsadow¹
(BatchNormalization) oraz operacje maksymalnego próbkowania (MaxPooling1D).
Warstwy splotowe wykorzystuj¹ funkcjê aktywacji ReLU i parametr padding=same,
który zachowuje rozmiar wyjœcia taki sam jak wejœcia. Rozmiary filtrów w
warstwach splotowych wynosz¹ odpowiednio 7, 5 i 3, a ich iloœæ to 64, 128 i
256. Maksymalne próbkowanie zmniejsza dwukrotnie rozmiar danych na koñcu
ka¿dego z bloków poprzez zastosowanie parametru pool\_size=2, oznaczaj¹cego, ¿e
dla ka¿dej pary s¹siaduj¹cych próbek wybierana jest wartoœæ maksymalna.

Po przejœciu przez pierwsze bloki przetwarzaj¹ce wyniki trafiaj¹ do warstwy
rekurencyjnej LSTM (Long Short-Term Memory), która zawiera 128 jednostek.
Wartswa ta przetwarza dane sekwencyjne, ucz¹c siê wzorców czasowych w sygnale.
Na wyjœciu warstwy LSTM zastosowano warstwê dropout na poziomie 30\%, co
oznacza, ¿e w trakcie treningu 30\% neuronów jest losowo dezaktywowanych.
Zapobiega to przeuczeniu modelu oraz poprawia jego zdolnoœci generalizacyjne.

Na koñcu model zawiera ostatni¹ warstwê gest¹, której liczba neuronów odpowiada
liczbie przewidywanych przez model wartoœci, na przyk³ad w przypadku
przewidywania SDNN i RMSSD rozmiar ten wynosi dwa. Warstwa ta wykorzystuje
liniowa funkcje aktywacji, co pozwala na generowanie ci¹g³ych wartoœci
wyjsciowych, zgodnych ze skala przewidywanych miar.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=1.5]{Rysunki/RR_siec.png}
    \caption{Schemat sieci typu feature-based do wyznaczania miar HRV}
    \label{fig/rr_siec}
\end{figure}

% \subsection{Model end-to-end do wyznaczania miar HRV}\label{siec-end-to-end}\
\FloatBarrier
\subsection{Model end-to-end}\label{siec-end-to-end}
W ramach pracy opracowano model sieci neuronowej typu end-to-end, bêdacy
rozwojow¹ wersj¹ prostego modelu opartego o interwa³y RR, opisanego w
podrozdziale \ref{rr-based}. Jego przeznaczeniem jest przewidywanie parametrów
HRV na podstawie surowego sygna³u EKG. Do jego implementacji wykorzystano
wysokopoziomowe API biblioteki \emph{tensorflow} o nazwie \emph{Keras}
\cite{TensorFlow-Keras}. Schemat architektury sieci przedstawiono na Rys.
\ref{fig/end-to-end}

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=1.5]{Rysunki/end-to-end.png}
    \caption{Schemat sieci end-to-end do wyznaczania miar HRV}
    \label{fig/end-to-end}
\end{figure}

Model u¿ywa takiej samej kombinacji bloków przetwarzaj¹cych co pierwowzór,
jednak ze zmodyfikowanymi parametrami. Rozmiary j¹der w warstwach
konwolucyjnych wynosz¹ kolejno 15, 11 i 9, a ich iloœæ zosta³a zmniejszona do
32, 64, oraz 128. Dodatkowo zmieniono parametr maksymalnego próbkowania: w
pierwszych dwóch warstwach zastosowano piêciokrotn¹ redukcjê wymiaru, a w
trzeciej warstwie czterokrotn¹.

Po przejœciu przez te trzy bloki, dane trafiaj¹ do warstwy uwagi,
zaimplementowanej przy u¿yciu warstwy gêstej (Dense) z funkcj¹ aktywacji
sigmoid. Wynik tej warstwy jest przemna¿any z wynikiem ostatniego bloku
przetwarzaj¹cego. Nastêpnie dane s¹ globalnie uœredniane
(GlobalAveragePooling1D), co redukuje je do wektora reprezentuj¹cego œrednie
wartoœci filtrów wzd³u¿ ca³ego sygna³u.

Po operacji uœredniania dane przechodz¹ przez dwie warstwy gêste, których
rozmiary wynosz¹ 128 i 64. Po ka¿dej z tych warstw zastosowano mechanizm
losowego odrzucania neuronów (dropout), odpowiednio na poziomie 40\% i 30\%.
Zapobiega to przeuczeniu modelu oraz wspiera jego generalizacje.

Ostatnia warstwa modelu pozosta³a niezmieniona w stosunku do pierwowzoru. Jest
to gêsta warstwa, której liczba neuronów odpowiada liczbie przewidywanych
wartoœci. Na przyk³ad, przy prognozowaniu SDNN i RMSSD, warstwa ta zawiera dwa
neurony. Wykorzystuje ona liniow¹ funkcjê aktywacji, co umo¿liwia modelowi
zwracanie ci¹g³ych wartoœci wyjœciowych, zgodnych z zakresem przewidywanych
parametrów.

\FloatBarrier

% W ramach pracy opracowano model sieci neuronowej typu end-to-end, bêdacy
% rozwojow¹ wersj¹ prostego modelu opartego o interwa³y RR z którego
% przeznaczeniem jest przewidywanie parametrów HRV na podstawie surowego sygna³u
% EKG. Do implementacji sieci wykorzystano wysokopoziomowe API biblioteki
% \emph{tensorflow} o nazwie \emph{Keras} \cite{TensorFlow-Keras}. Schemat
% architektury sieci przedstawiono na Rys. \ref{fig/end-to-end}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[scale=1.5]{Rysunki/end-to-end.png}
%     \caption{Schemat sieci end-to-end do wyznaczania miar HRV}
%     \label{fig/end-to-end}
% \end{figure}

% Model oparto o kaskade trzech bloków przetwarzania, z których ka¿dy zawiera
% jednowymiarowa warstwê splotow¹ (Conv1D), normalizacje wsadow¹
% (BatchNormalization) oraz operacje maksymalnego próbkowania (MaxPooling1D).
% Warstwy splotowe wykorzystuj¹ funkcjê aktywacji ReLU i parametr padding=same,
% który zachowuje rozmiar wyjœcia taki sam jak wejœcia. Rozmiary filtrów w
% warstwach splotowych wynosz¹ odpowiednio 15, 11 i 9, a ich iloœæ to 32, 64 i
% 128. Maksymalne próbkowanie zmniejsza rozmiar danych na koniec z ka¿dego z
% bloków, pieciokrotnie w przypadku pierwszych dwóch oraz czterokrotnie w wypadku
% trzeciego.

% Po przejœciu przez te trzy bloki, dane trafiaj¹ do warstwy uwagi,
% zaimplementowanej przy u¿yciu warstwy gêstej (Dense) z funkcj¹ aktywacji
% sigmoid. Wynik tej warstwy jest przemna¿any z wynikiem ostatniego bloku
% przetwarzaj¹cego. Nastêpnie dane s¹ globalnie uœredniane
% (GlobalAveragePooling1D), co redukuje je do wektora reprezentuj¹cego œrednie
% wartoœci filtrów wzd³u¿ ca³ego sygna³u.

% Po operacji uœredniania dane przechodz¹ przez dwie warstwy gêste, których
% rozmiary wynosz¹ 128 i 64. Po ka¿dej z tych warstw zastosowano mechanizm
% losowego odrzucania neuronów (dropout), odpowiednio na poziomie 40\% i 30\%.
% Zapobiega to przeuczeniu modelu oraz wspiera jego generalizacje.

% Na koñcu model zawiera ostatni¹ warstwê gest¹, której liczba neuronów odpowiada
% liczbie przewidywanych przez model wartoœci, na przyk³ad w przypadku
% przewidywania SDNN i RMSSD rozmiar ten wynosi dwa. Warstwa ta wykorzystuje
% liniowa funkcje aktywacji.
